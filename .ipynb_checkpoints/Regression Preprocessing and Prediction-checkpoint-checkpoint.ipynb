{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data options\n",
    "option 1: generate data by using the op and cp range\n",
    "         - combine data of the op range and determine if there is a cp output\n",
    "option 2: have a range for every record \n",
    "         - example: 1. looks at record+1 and if record within time range is true then identiyf whether it is a churned user."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# parallelization example( ITS NOT FASTER BECAUSE EACH LOOP IS NOT COMPUTATIONALY EXPENSIVE.)\n",
    "from joblib import Parallel, delayed\n",
    "#ADD METHODS TO A NEW PYTHON FILE- REQUIRED DUE A KNOWN WINDOWS BUG\n",
    "from trialss_data import dis_simple,get_dat_at_time,get_binned_data,gen_reg_data\n",
    "#THIS IS NECESSARY TO MAKE SURE OUR DATA IS NOT LOST IN MEMORY\n",
    "if __name__ == \"__main__\":\n",
    "    random_state=42\n",
    "    op=5\n",
    "    cp=10\n",
    "    bin_time=30\n",
    "    include_team=False\n",
    "    data=get_preproc_data(include_team)\n",
    "   # display(data)\n",
    "    data=data[['player_name','date','player_score']]\n",
    "    #data=gen_parallel_reg_data(preprocessed_data,op=5,cp=10,bin_time=30)\n",
    "    new_data=[]\n",
    "    new_data=Parallel(n_jobs=4)(delayed(player_reg_data)(data,x,op,cp,bin_time) for x in tqdm(data['player_name'].unique()))\n",
    "\n",
    "    reg_data=pd.DataFrame(new_data)\n",
    "    reg_data.rename(columns={ reg_data.columns[-1]: 'churned' })    \n",
    "    #data=get_reg_data(preprocessed_data.copy(),op,cp,bin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import  accuracy_score,classification_report,roc_auc_score,log_loss,zero_one_loss,confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def feature_plot(imp_features,X):\n",
    "    indices = np.argsort(imp_features)[::-1]\n",
    "    num_features=len(imp_features[imp_features>0])\n",
    "    columns = X.columns.values[indices][:num_features]\n",
    "    values=imp_features[indices][:num_features]\n",
    "\n",
    "    plt.figure(figsize = (15,5))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.barh(range(num_features), values, align=\"center\")\n",
    "    plt.yticks(range(num_features), columns)\n",
    "    plt.ylim([ num_features,-1])\n",
    "    plt.show() \n",
    "\n",
    "def classification_metric(y_true,y_predict):\n",
    "    print('accuracy score',accuracy_score(y_true,y_predict.round()))\n",
    "    print('roc_auc_score',roc_auc_score(y_true, y_predict.round()))\n",
    "    print('log loss score',log_loss(y_true,y_predict))\n",
    "    print('zero_one_loss score',zero_one_loss(y_true,y_predict.round()))\n",
    "    print('confusion_matrix',confusion_matrix(y_true,y_predict.round()))\n",
    "    print(classification_report(y_true,y_predict.round()))\n",
    "    \n",
    "\n",
    "def get_preproc_data(include_team=False):\n",
    "    if include_team:\n",
    "        preprocessed_data=pickle.load(open('processed_raw_data_with_team.p', \"rb\" ))\n",
    "    else:\n",
    "        preprocessed_data=pickle.load(open('processed_raw_data.p', \"rb\" ))\n",
    "        \n",
    "    return preprocessed_data\n",
    "def filter_columns(data,columns,to_filter=True):\n",
    "    if to_filter:\n",
    "        return data[columns]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "def scale_numerical_data(data,scale=False):\n",
    "    if scale: \n",
    "        scaler=MinMaxScaler()\n",
    "        #scaler=StandardScaler()\n",
    "        columns_to_encode=list(data.select_dtypes(include=['float64','int64']))\n",
    "        columns_to_encode.remove('churned')\n",
    "\n",
    "\n",
    "       # skewed=['avg_game_duration','avg_dist_visit','avg_player_degree','avg_team_score','has_group',\n",
    "       #         'max_duration','max_dist_visit']\n",
    "       # features_log_transformed = pd.DataFrame(data = data)\n",
    "       # features_log_transformed[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "        features_transform = pd.DataFrame(data = data)\n",
    "        features_transform[columns_to_encode] = scaler.fit_transform(data[columns_to_encode])\n",
    "        return features_transform\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_classification_results(X_train,y_train,X_test,y_test,clf_models,print_var_imp):\n",
    "    for clf in clf_models:\n",
    "        print('classifier: ',clf.__class__.__name__)\n",
    "        clf.fit(X_train,y_train)\n",
    "        #imp_features=clf.feature_importances_\n",
    "        #feature_plot(imp_features,X)\n",
    "        pred=clf.predict(X_test)\n",
    "        classification_metric(y_test,pred)\n",
    "        \n",
    "        if print_var_imp:\n",
    "            try:\n",
    "                imp_features=clf.feature_importances_\n",
    "                feature_plot(imp_features,X)  \n",
    "            except AttributeError:\n",
    "                pass\n",
    "        \n",
    "def print_feature_importance(X,y,clf_models):\n",
    "    for clf in clf_models:\n",
    "        print('classifier: ',clf.__class__.__name__)\n",
    "        clf.fit(X,y)\n",
    "        imp_features=clf.feature_importances_\n",
    "        feature_plot(imp_features,X)\n",
    "        \n",
    "\n",
    "    \n",
    "def player_reg_data(data,x,op,cp,bin_time):\n",
    "        player_data=data.loc[data['player_name']==x]\n",
    "        player_data.set_index('date',inplace=True)\n",
    "        op_min_date=player_data.index.min()\n",
    "        op_max_date=op_min_date+ pd.DateOffset(days=op)\n",
    "        cp_max_date=op_max_date+ pd.DateOffset(days=cp)\n",
    "        date_range=pd.date_range(start=op_min_date, end=op_max_date,freq=str(bin_time)+'T',closed=None)\n",
    "        df2=pd.DataFrame({ 'player_score': 0 * len(date_range) },index=date_range )\n",
    "        res_player_data=player_data.append(df2)\n",
    "        res_player_data=res_player_data.resample(str(bin_time)+'T',label='right', closed=None).sum()\n",
    "        res_player_data=res_player_data.loc[(res_player_data.index>=op_min_date)&(res_player_data.index<=op_max_date)]\n",
    "        col_data=res_player_data['player_score'].tolist()\n",
    "        cp_data=player_data.loc[(player_data.index > op_max_date) & (player_data.index<= cp_max_date)]\n",
    "        col_data.append(1 if cp_data.empty else 0)\n",
    "        return col_data\n",
    "    \n",
    "def gen_reg_data(data,op,cp,bin_time):\n",
    "    #new_data=[]\n",
    "    new_data=[player_reg_data(data,player,op,cp,bin_time) for player in tqdm(data['player_name'].unique())]\n",
    "    #new_data.append(player_reg_data(data,player,op,cp,bin_time))\n",
    "    reg_data=pd.DataFrame(new_data)\n",
    "    return reg_data.rename(columns={ reg_data.columns[-1]: 'churned' })    \n",
    "\n",
    "\n",
    "def get_reg_data(data,op,cp,bin_time):\n",
    "    name='reg_data_op_'+str(op)+'_'+str(cp)+'_bin_'+str(bin_time)+'.p'\n",
    "    if os.path.isfile(name):\n",
    "        data=pickle.load(open(name,'rb'))\n",
    "    else:\n",
    "        data=gen_reg_data(preprocessed_data.copy(),op,cp,bin_time)\n",
    "        pickle.dump(data,open(name, \"wb\" ))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Term Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>churned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1  2  3  4  5  6  7  8  9   ...     231  232  233  234  235  236  237  \\\n",
       "0  19   0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "1  50   0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "2  56  31  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "\n",
       "   238  239  churned  \n",
       "0    0    0        0  \n",
       "1    0    0        1  \n",
       "2    0    0        0  \n",
       "\n",
       "[3 rows x 241 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state=42\n",
    "op=5\n",
    "cp=10\n",
    "bin_time=30\n",
    "scale_data=False\n",
    "include_team=False\n",
    "preprocessed_data=get_preproc_data(include_team)\n",
    "data=get_reg_data(preprocessed_data.copy(),op,cp,bin_time)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (13166, 240, 1) x_test (1463, 240, 1)\n"
     ]
    }
   ],
   "source": [
    "data=scale_numerical_data(data,scale_data)\n",
    "\n",
    "X=data.drop(['churned'],axis=1)\n",
    "X=X.values\n",
    "X= X.reshape((-1,X.shape[1], 1))\n",
    "y=data['churned']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=random_state)\n",
    "print('x_train',X_train.shape, 'x_test',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7758031442241968\n",
      "roc_auc_score 0.7373346103796383\n",
      "log loss score 0.511901501175309\n",
      "zero_one_loss score 0.22419685577580317\n",
      "confusion_matrix [[317 214]\n",
      " [114 818]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.60      0.66       531\n",
      "          1       0.79      0.88      0.83       932\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import CuDNNLSTM,CuDNNGRU,BatchNormalization, Input,Bidirectional,TimeDistributed,RepeatVector\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential,Model\n",
    "\n",
    "\n",
    "def build_simple_model(layers):\n",
    "    model = Sequential()\n",
    "    #input shape follows the (sequence length,time)\n",
    "    #input shape (Sequence length(50),num of features(1)), neurons=(seq len)50\n",
    "    model.add(CuDNNLSTM( input_shape=(layers[1], layers[0]), units=128))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\n",
    "                                                                \"binary_accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_bidir_model(layers):\n",
    "    inp_seq=Input(shape=(layers[1], layers[0]))\n",
    "    rnn=Bidirectional(CuDNNGRU(256,return_sequences=True),merge_mode='concat')(inp_seq)\n",
    "    drp_out = Dropout(0.2)(rnn)\n",
    "    bidir_rnn = Bidirectional(CuDNNGRU(256))(drp_out)\n",
    "    drp_out = Dropout(0.2)(bidir_rnn)\n",
    "    logits=Dense(1)(drp_out)\n",
    "    logits=Activation(\"sigmoid\")(logits)\n",
    "    \n",
    "    #cost=Activation('softmax')(logits)\n",
    "    model=Model(inp_seq,logits)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\n",
    "                                                                \"binary_accuracy\"])\n",
    "    return model\n",
    "\n",
    "def bi_deep_dense_model(layers):\n",
    "    inp_seq=Input(shape=(layers[1], layers[0]))\n",
    "    rnn=Bidirectional(CuDNNGRU(256,return_sequences=True),merge_mode='concat')(inp_seq)\n",
    "    drp_out = Dropout(0.2)(rnn)\n",
    "    bidir_rnn = Bidirectional(CuDNNGRU(256))(drp_out)\n",
    "    drp_out = Dropout(0.2)(bidir_rnn)\n",
    "    hid_dense=Dense(10)(drp_out)\n",
    "    drp_out = Dropout(0.2)(hid_dense)\n",
    "    logits=Dense(1)(drp_out)\n",
    "    logits=Activation(\"sigmoid\")(logits)\n",
    "    \n",
    "    #cost=Activation('softmax')(logits)\n",
    "    model=Model(inp_seq,logits)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\n",
    "                                                                \"binary_accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def bi_norm_deep_dense_model(layers):\n",
    "    inp_seq=Input(shape=(layers[1], layers[0]))\n",
    "    rnn=Bidirectional(CuDNNGRU(256,return_sequences=True),merge_mode='concat')(inp_seq)\n",
    "    drp_out = Dropout(0.2)(rnn)\n",
    "    bidir_rnn = Bidirectional(CuDNNGRU(256))(drp_out)\n",
    "    drp_out = Dropout(0.2)(bidir_rnn)\n",
    "    hid_dense=Dense(10)(drp_out)\n",
    "    drp_out = BatchNormalization()(hid_dense)\n",
    "    logits=Dense(1)(drp_out)\n",
    "    logits=Activation(\"sigmoid\")(logits)\n",
    "    \n",
    "    #cost=Activation('softmax')(logits)\n",
    "    model=Model(inp_seq,logits)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[ \"binary_accuracy\", 'sparse_categorical_accuracy' ])\n",
    "    return model\n",
    "\n",
    "def test_bi_norm_deep_dense_model(layers):\n",
    "    inp_seq=Input(shape=(layers[1], layers[0]))\n",
    "    rnn=CuDNNGRU(256,return_sequences=True)(inp_seq)\n",
    "    drp_out = Dropout(0.2)(rnn)\n",
    "    rnn=CuDNNGRU(256,return_sequences=True)(drp_out)\n",
    "    drp_out = Dropout(0.2)(rnn)\n",
    "    bidir_rnn = CuDNNGRU(256)(drp_out)\n",
    "    drp_out = Dropout(0.2)(bidir_rnn)\n",
    "    hid_dense=Dense(10)(drp_out)\n",
    "    drp_out = BatchNormalization()(hid_dense)\n",
    "    logits=Dense(1)(drp_out)\n",
    "    logits=Activation(\"sigmoid\")(logits)\n",
    "    \n",
    "    #cost=Activation('softmax')(logits)\n",
    "    model=Model(inp_seq,logits)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=[ \"binary_accuracy\", 'sparse_categorical_accuracy' ])\n",
    "    return model\n",
    "\n",
    "def norm_2_deep_dense_model(layers):\n",
    "    inp_seq=Input(shape=(layers[1], layers[0]))\n",
    "    rnn=CuDNNGRU(128,return_sequences=True)(inp_seq)\n",
    "    drp_out = Dropout(0.2)(rnn)\n",
    "    bidir_rnn = CuDNNGRU(256)(drp_out)\n",
    "    drp_out = Dropout(0.2)(bidir_rnn)\n",
    "    hid_dense=Dense(10)(drp_out)\n",
    "    drp_out = Dropout(0.2)(hid_dense)\n",
    "    hid_dense=Dense(10)(drp_out)\n",
    "    drp_out = BatchNormalization()(hid_dense)\n",
    "    logits=Dense(1)(drp_out)\n",
    "    logits=Activation(\"sigmoid\")(logits)\n",
    "    \n",
    "    #cost=Activation('softmax')(logits)\n",
    "    model=Model(inp_seq,logits)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=[ \"binary_accuracy\", 'sparse_categorical_accuracy' ])\n",
    "    return model\n",
    "    \n",
    "batch_size=256\n",
    "epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_bi_norm_deep_dense_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7819548872180451\n",
      "roc_auc_score 0.7441886310548564\n",
      "log loss score 0.5102876363460914\n",
      "zero_one_loss score 0.21804511278195493\n",
      "confusion_matrix [[322 209]\n",
      " [110 822]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.61      0.67       531\n",
      "          1       0.80      0.88      0.84       932\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = bi_deep_dense_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "epochs=50\n",
    "model = bi_deep_dense_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7662337662337663\n",
      "roc_auc_score 0.7176697137961413\n",
      "log loss score 0.5142230513089948\n",
      "zero_one_loss score 0.23376623376623373\n",
      "confusion_matrix [[287 244]\n",
      " [ 98 834]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.54      0.63       531\n",
      "          1       0.77      0.89      0.83       932\n",
      "\n",
      "avg / total       0.76      0.77      0.76      1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_simple_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7764866712235133\n",
      "roc_auc_score 0.7378710910663338\n",
      "log loss score 0.5110068828566477\n",
      "zero_one_loss score 0.22351332877648666\n",
      "confusion_matrix [[317 214]\n",
      " [113 819]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.60      0.66       531\n",
      "          1       0.79      0.88      0.83       932\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_bidir_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7710184552289815\n",
      "roc_auc_score 0.7270970231888978\n",
      "log loss score 0.5166401379994452\n",
      "zero_one_loss score 0.2289815447710185\n",
      "confusion_matrix [[301 230]\n",
      " [105 827]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.57      0.64       531\n",
      "          1       0.78      0.89      0.83       932\n",
      "\n",
      "avg / total       0.77      0.77      0.76      1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = bi_norm_deep_dense_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7662337662337663\n",
      "roc_auc_score 0.7140234637052125\n",
      "log loss score 0.5151478937939726\n",
      "zero_one_loss score 0.23376623376623373\n",
      "confusion_matrix [[278 253]\n",
      " [ 89 843]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.52      0.62       531\n",
      "          1       0.77      0.90      0.83       932\n",
      "\n",
      "avg / total       0.76      0.77      0.75      1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = norm_2_deep_dense_model([1, X.shape[1]])\n",
    "history=model.fit(X_train,y_train,batch_size=batch_size,shuffle=True,epochs=epochs,validation_split=0.20,verbose=0)\n",
    "pred=model.predict(X_test)\n",
    "classification_metric(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
